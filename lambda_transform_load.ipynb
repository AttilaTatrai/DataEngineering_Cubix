{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from io import StringIO\n",
    "\n",
    "import json\n",
    "import boto3\n",
    "import pandas as pd\n",
    "\n",
    "def taxi_trips_transformations(taxi_trips: pd.DataFrame)->pd.DataFrame:\n",
    "    \"\"\"Perform transformations with the taxi data.\n",
    "    \n",
    "    Args:\n",
    "        taxi_trips (pd.DataFrame): \n",
    "        The DataFrame holding the daily taxi trips.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: \n",
    "        The cleaned, transformed DataFrame holding the daily taxi trips.\n",
    "    \"\"\"\n",
    "\n",
    "    if not isinstance(taxi_trips, pd.DataFrame):\n",
    "        raise TypeError(\"taxi_trips is not a valid pandas DataFrame.\")\n",
    "    \n",
    "    taxi_trips.drop([\"pickup_census_tract\", \"dropoff_census_tract\", \n",
    "                     \"pickup_centroid_location\",\"dropoff_centroid_location\"], axis = 1, inplace =True)\n",
    "\n",
    "    taxi_trips.dropna(inplace=True)\n",
    "\n",
    "    taxi_trips.rename(columns={\"pickup_community_area\" : \"pickup_community_area_id\",\n",
    "                            \"dropoff_community_area\": \"dropoff_community_area_id\"}, inplace = True)\n",
    "\n",
    "    taxi_trips[\"datetime_for_weather\"] = pd.to_datetime(taxi_trips[\"trip_start_timestamp\"]).dt.floor(\"h\")\n",
    "\n",
    "    return taxi_trips\n",
    "    \n",
    "def update_taxi_trips_with_master_data(taxi_trips: pd.DataFrame, payment_type_master: pd.DataFrame, company_master: pd.DataFrame)->pd.DataFrame:\n",
    "    \"\"\"Update the taxi_trips DataFrame with company_mater and payment_type_master ids, and delete string columns.\n",
    "\n",
    "    Args:\n",
    "        taxi_trips (pd.DataFrame): \n",
    "        The data with the daily taxi trips.\n",
    "\n",
    "        payment_type_master (pd.DataFrame):\n",
    "         The payment type master table.\n",
    "        company_master (pd.DataFrame):\n",
    "         The company master table.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: \n",
    "        Taxi trips data, with only payment_type_id and company_id, without company or payment_type values.\n",
    "    \"\"\"\n",
    "\n",
    "    taxi_trips_id = taxi_trips.merge(payment_type_master, on =\"payment_type\")\n",
    "    taxi_trips_id = taxi_trips_id.merge(company_master, on =\"company\")\n",
    "\n",
    "    taxi_trips_id.drop([\"payment_type\", \"company\"], axis =1, inplace=True)\n",
    "\n",
    "    return taxi_trips_id    \n",
    "    \n",
    "def update_master(taxi_trips: pd.DataFrame, master: pd.DataFrame, id_column: str, value_column: str)->pd.DataFrame:\n",
    "    \"\"\"Extend the  master DataFrame with new values nt if there are any new parameters.\n",
    "\n",
    "    Args:\n",
    "        taxi_trips (pd.DataFrame): \n",
    "        DataFrame holding the daily taxi trips.\n",
    "        company_master (pd.DataFrame): \n",
    "        DataFrame holding the master data.\n",
    "    id_column: str\n",
    "        The id column of the master DataFrame.\n",
    "    value_column: str\n",
    "        Name of the column in master_df containing the values.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: \n",
    "        The updated master data, if new values types are in the taxi data, they will be loaded to it.\n",
    "    \"\"\"\n",
    "\n",
    "    max_id = master[id_column].max()\n",
    "\n",
    "    new_values_list= [value for value in taxi_trips[value_column].values if value not in master[value_column].values]\n",
    "    new_values_df = pd.DataFrame({\n",
    "        id_column: range(max_id +1, max_id + len(new_values_list)+1),\n",
    "        value_column: new_values_list\n",
    "    })\n",
    "        \n",
    "    updated_master = pd.concat([master,new_values_df], ignore_index=True)\n",
    "    return updated_master\n",
    "\n",
    "def transform_weather_data(weather_data:json)->pd.DataFrame:\n",
    "    \"\"\"Make transformations on the daily weather api resonpse.\n",
    "\n",
    "    Args:\n",
    "        weather_data (json): The daily weather data from the Opne Meteo API.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: A DataFrame representation of the data.\n",
    "    \"\"\"\n",
    "    #Transform part:\n",
    "    weather_data_filtered = {\n",
    "    \"datetime\": weather_data[\"hourly\"][\"time\"],\n",
    "    \"temperature\": weather_data[\"hourly\"][\"temperature_2m\"],\n",
    "    \"windspeed\": weather_data[\"hourly\"][\"wind_speed_10m\"],\n",
    "    \"rain\": weather_data[\"hourly\"][\"rain\"],\n",
    "    \"precipitation\": weather_data[\"hourly\"][\"precipitation\"],\n",
    "\n",
    "    }\n",
    "\n",
    "    weather_df = pd.DataFrame(weather_data_filtered)\n",
    "    weather_df[\"datetime\"] = pd.to_datetime(weather_df[\"datetime\"])\n",
    "    return weather_df\n",
    "    \n",
    "def read_csv_from_s3(bucket: str, path: str, filename:str)->pd.DataFrame:\n",
    "    \"\"\" Dwonloads a csv file from an S3 bucket.\n",
    "    Parameters\n",
    "    ---------\n",
    "    bucket: str\n",
    "    The bucket is where the files at.\n",
    "    path: str\n",
    "    The folders to the file.\n",
    "    filename: str\n",
    "    Name of the file.\n",
    "    Retuns: \n",
    "    ------\n",
    "    pd.DataFrame\n",
    "    A DataFrame of the downloaded file.\n",
    "    \"\"\"\n",
    "    s3 = boto3.client(\"s3\")\n",
    "    full_path = f\"{path}{filename}\"\n",
    "    \n",
    "    object = s3.get_object(Bucket = bucket, Key = full_path)\n",
    "    object = object[\"Body\"].read().decode(\"utf-8\")\n",
    "    output_df = pd.read_csv(StringIO(object))\n",
    "    return output_df\n",
    "\n",
    "def upload_dataframe_to_s3(dataframe: pd.DataFrame, bucket: str, path : str):\n",
    "    \"\"\"\n",
    "    Uploads a dataframe to the specified S3 path.\n",
    "    Parameters\n",
    "    ---------\n",
    "    dataframe: pd.DataFrame\n",
    "    The dataframe to be uploaded\n",
    "    bucket: str\n",
    "    Name of the S3 bucket where we want to store our data.\n",
    "    path: str\n",
    "    Path within the bucket ot upload files.\n",
    "    Retuns\n",
    "    ------\n",
    "    None\n",
    "    \"\"\"\n",
    "    s3 = boto3.client(\"s3\")\n",
    "    buffer = StringIO()\n",
    "    dataframe.to_csv(buffer, index = False)\n",
    "    df_content = buffer.getvalue()\n",
    "    s3.put_object(Bucket = bucket, Key = path, Body = df_content)\n",
    "    \n",
    "def upload_master_data_to_s3(bucket: str, path: str, file_type:str, dataframe: pd.DataFrame):\n",
    "    \"\"\"\n",
    "    Uploads master data (payment_type or company) to S3. Copies the previous versions and creates the new one.\n",
    "    Parameters \n",
    "    -----------\n",
    "    \n",
    "    bucket: str\n",
    "    Name of the S3 bucket, where we want to store the files.\n",
    "    path: strPath withtin the bucket to upload the files.\n",
    "    file_type: str\n",
    "    Either \"company\" or payment_type.\n",
    "    dataframe: pd.DataFrame\n",
    "    The dataframe to be uploaded.\n",
    "    \n",
    "    Retuns\n",
    "    ------\n",
    "    None\n",
    "    \"\"\"\n",
    "    \n",
    "    s3 = boto3.client(\"s3\")\n",
    "    master_file_path = f\"{path}{file_type}_master.csv\"\n",
    "    previous_master_file_path = f\"transformed_data/master_table_previous_version/{file_type}_master_previous_version.csv\"\n",
    "    \n",
    "    s3.copy_object(\n",
    "        Bucket=bucket,\n",
    "        CopySource = {\"Bucket\":bucket, \"Key\": master_file_path},\n",
    "        Key = previous_master_file_path\n",
    "        )\n",
    "        \n",
    "    buffer = StringIO()\n",
    "    dataframe.to_csv(buffer,index=False)\n",
    "    df_content = buffer.getvalue()\n",
    "    s3.put_object(Bucket=bucket, Key=master_file_path, Body=df_content)\n",
    "    upload_dataframe_to_s3(bucket = bucket, dataframe=dataframe, path = master_file_path)\n",
    "    \n",
    "def upload_and_move_file_on_s3(\n",
    "    dataframe:pd.DataFrame,\n",
    "    datetime_col:str,\n",
    "    bucket : str,\n",
    "    target_path_transformed: str,\n",
    "    file_type:str,\n",
    "    source_path: str,\n",
    "    target_path_raw: str,\n",
    "    filename: str\n",
    "   ):\n",
    "    \"\"\"\n",
    "    Uploads a DataFrame to an S3 bucket and moves a specified file within the S3 bucket.\n",
    "\n",
    "    This function performs the following actions:\n",
    "    1. Formats the date from the specified datetime column in the DataFrame.\n",
    "    2. Creates a new path with filename based on the date and the specified file type.\n",
    "    3. Uploads the DataFrame to the S3 bucket at the newly created path.\n",
    "    4. Copies a file from the source path to the target raw path within the S3 bucket.\n",
    "    5. Deletes the original file from the source path in the S3 bucket.\n",
    "\n",
    "    Args:\n",
    "    dataframe (pd.DataFrame): The DataFrame to be uploaded to S3.\n",
    "    datetime_col (str): The column name in the DataFrame containing datetime information.\n",
    "    bucket (str): The name of the S3 bucket.\n",
    "    target_path_transformed (str): The target path for storing the transformed DataFrame in the S3 bucket.\n",
    "    file_type (str): The file type to be used in the new filename (e.g., 'csv').\n",
    "    source_path (str): The source path in the S3 bucket from which the file will be copied.\n",
    "    target_path_raw (str): The target path in the S3 bucket to which the file will be copied.\n",
    "    filename (str): The name of the file to be copied and deleted from the source path.\n",
    "\n",
    "    Returns:\n",
    "    None\n",
    "    \"\"\"\n",
    "    s3 = boto3.client(\"s3\")\n",
    "    formatted_date = dataframe[datetime_col].iloc[0].strftime(\"%Y-%m-%d\")\n",
    "    new_path_with_filename = f\"{target_path_transformed}{file_type}_{formatted_date}.csv\"\n",
    "    \n",
    " \n",
    "    upload_dataframe_to_s3(bucket = bucket, dataframe=dataframe, path = new_path_with_filename)\n",
    "    \n",
    "    s3.copy_object(\n",
    "        Bucket = bucket,\n",
    "        CopySource = {\"Bucket\":bucket, \"Key\":f\"{source_path}{filename}\"},\n",
    "        Key = f\"{target_path_raw}{filename}\"\n",
    "    )\n",
    "    s3.delete_object(Bucket=bucket, Key =f\"{source_path}{filename}\")\n",
    "    \n",
    "def lambda_handler(event, context):\n",
    "    s3 = boto3.client(\"s3\")\n",
    "    \n",
    "    bucket = \"cubix-chichago-taxi-ta\"\n",
    "    raw_weather_folder = \"raw_data/to_processed/weather_data/\"\n",
    "    raw_taxi_trips_folder = \"raw_data/to_processed/taxi_data/\"\n",
    "    target_taxi_trips_foler =\"raw_data/processed/taxi_data/\" \n",
    "    target_weather_folder = \"raw_data/processed/weather_data/\"\n",
    "    \n",
    "    transformed_taxi_trips_folder = \"transformed_data/taxi_trips/\"\n",
    "    transformed_weather_folder = \"transformed_data/weather/\"\n",
    "    \n",
    "    \n",
    "    payment_type_master_folder = \"transformed_data/payment_type/\"\n",
    "    company_master_folder = \"transformed_data/company/\"\n",
    "    \n",
    "    \n",
    "    payment_type_master_file_name = \"payment_type_master.csv\"\n",
    "    company_master_file_name = \"company_master_.csv\"\n",
    "    \n",
    "    \n",
    "    payment_type_master = read_csv_from_s3(bucket=bucket,path=payment_type_master_folder, filename = payment_type_master_file_name)\n",
    "    company_master = read_csv_from_s3(bucket=bucket, path=company_master_folder, filename = company_master_file_name)\n",
    "    \n",
    "\n",
    "    #TAXI DATA TRANSFORMATIKON AND LOADING\n",
    "    for file in s3.list_objects(Bucket = bucket, Prefix = raw_taxi_trips_folder )[\"Contents\"]:\n",
    "        taxi_trip_key = file[\"Key\"]\n",
    "                \n",
    "        if taxi_trip_key.split(\"/\")[-1].strip() !=\"\":\n",
    "            if taxi_trip_key.split(\".\")[1] == \"json\":\n",
    "                filename = taxi_trip_key.split(\"/\")[-1]\n",
    "                response = s3.get_object(Bucket=bucket, Key=taxi_trip_key)\n",
    "                content = response[\"Body\"]\n",
    "                taxi_trips_data_json = json.loads(content.read())\n",
    "                    \n",
    "                taxi_trips_data_raw = pd.DataFrame(taxi_trips_data_json)\n",
    "                taxi_trips_transformed = taxi_trips_transformations(taxi_trips_data_raw)\n",
    "                        \n",
    "                company_master_updated = update_master(taxi_trips_transformed, company_master, \"company_id\", \"company\")\n",
    "                payment_type_master_updated = update_master(taxi_trips_transformed, payment_type_master,\"payment_type_id\",  \"payment_type\")\n",
    "                        \n",
    "                        \n",
    "                taxi_trips = update_taxi_trips_with_master_data(taxi_trips_transformed, payment_type_master_updated, company_master_updated)\n",
    "                upload_and_move_file_on_s3(\n",
    "                            \n",
    "                    dataframe=taxi_trips,\n",
    "                    datetime_col=\"datetime_for_weather\",\n",
    "                    bucket = bucket,\n",
    "                    target_path_transformed = transformed_taxi_trips_folder,\n",
    "                    file_type=\"taxi\",\n",
    "                    source_path= raw_taxi_trips_folder,\n",
    "                    target_path_raw = target_taxi_trips_foler,\n",
    "                    filename= filename\n",
    "       \n",
    "                    )\n",
    "                print(\"taxi_trips is uploaded and moved.\")\n",
    "    \n",
    "                upload_master_data_to_s3(bucket=bucket, path = payment_type_master_folder, file_type = \"payment_type\", dataframe = payment_type_master_updated)\n",
    "                print(\"payment_type master has been updated\")\n",
    "                upload_master_data_to_s3(bucket=bucket, path = company_master_folder, file_type = \"company\", dataframe = company_master_updated)\n",
    "                print(\"company master has been updated\")\n",
    "             \n",
    "    #WEATHER DATA TRANSFORMATION AND LOADING\n",
    "    for file in s3.list_objects(Bucket = bucket, Prefix = raw_weather_folder )[\"Contents\"]:\n",
    "        weather_key = file[\"Key\"]\n",
    "        \n",
    "        if weather_key.split(\"/\")[-1].strip() !=\"\":\n",
    "            if weather_key.split(\".\")[1] == \"json\":\n",
    "                filename = weather_key.split(\"/\")[-1]\n",
    "                \n",
    "                response = s3.get_object(Bucket=bucket, Key=weather_key)\n",
    "                content = response[\"Body\"]\n",
    "                weather_data_json = json.loads(content.read())\n",
    "                \n",
    "                weather_data = transform_weather_data(weather_data_json)\n",
    "                upload_and_move_file_on_s3(\n",
    "                            \n",
    "                    dataframe=weather_data,\n",
    "                    datetime_col=\"datetime\",\n",
    "                    bucket = bucket,\n",
    "                    target_path_transformed = transformed_weather_folder,\n",
    "                    file_type=\"weather\",\n",
    "                    source_path= raw_weather_folder,\n",
    "                    target_path_raw = target_weather_folder,\n",
    "                    filename= filename\n",
    "                    )\n",
    "                \n",
    "                \n",
    "                print(weather_data[\"datetime\"].iloc[0].strftime(\"%Y-%m-%d\"))\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
